{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: \n",
    "- https://www.kaggle.com/datafiniti/grammar-and-online-product-reviews/data\n",
    "- https://www.kaggle.com/arathee2/demonetization-in-india-twitter-data/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from utility.preprocessing import preprocessing\n",
    "preprocessing = partial(preprocessing, HYPHEN_HANDLE=2, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(corpus, _slice=3):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    corpus = tokenizer.texts_to_sequences(corpus)\n",
    "    \n",
    "    data = []\n",
    "    targets = []\n",
    "    for sentence in tqdm(corpus):\n",
    "        slices = [sentence[i: i+_slice] for i in range(0, len(sentence) - (_slice-1))]\n",
    "        center = int(np.floor(_slice/2))\n",
    "        for s in slices:\n",
    "            data.append([s[center]])\n",
    "            targets.append([_ for idx, _ in enumerate(s) if idx != center])\n",
    "    \n",
    "    X = np.zeros((len(data), len(tokenizer.word_index)+1))\n",
    "    y = np.zeros((len(data), len(tokenizer.word_index)+1))\n",
    "    for idx, (i, j) in enumerate(zip(data, targets)):\n",
    "        X[idx][i] = 1\n",
    "        y[idx][j] = 1\n",
    "\n",
    "    print(\"X_shape:\", X.shape)\n",
    "    print(\"y_shape:\", y.shape)\n",
    "    print(\"# Words:\", len(tokenizer.word_index))\n",
    "\n",
    "    return X, y, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('./data/demonetization-tweets.csv', encoding='latin-1', usecols=['text'])\n",
    "df_data.drop_duplicates(inplace=True)\n",
    "df_data.dropna(inplace=True)\n",
    "df_data.text = df_data.text.apply(preprocessing)\n",
    "corpus = [_ for sent in df_data.text.tolist() for _ in sent.split(\".\")]\n",
    "X, y, tokenizer = generate_data(corpus, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(2, input_shape=(X.shape[1],)),\n",
    "    Dense(X.shape[1]),\n",
    "    Activation('softmax')\n",
    "])\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    h = model.fit(X, y, epochs=100, verbose=1)\n",
    "except KeyboardInterrupt:\n",
    "    print('\\n\\nExited by User')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = model.layers[0].get_weights()[0]\n",
    "word_embedding = {word: embedding for word, embedding in zip(tokenizer.word_index.keys(), points[1:])}\n",
    "inverse_idx = {v: k for k, v in tokenizer.word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest(word, _top=5):\n",
    "    word = word_embedding[word]\n",
    "    cos_sim = cosine_similarity(word.reshape(1, -1), points)\n",
    "    top_n = cos_sim.argsort()[0][-_top:][::-1]\n",
    "    return [inverse_idx[_] for _ in top_n if _ in inverse_idx]\n",
    "\n",
    "def similarity(word_1, word_2):\n",
    "    return cosine_similarity(\n",
    "        word_embedding[word_1].reshape(1, -1), \n",
    "        word_embedding[word_2].reshape(1, -1)\n",
    "    ).flatten()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity('atm', 'bank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt_x = points.transpose()[0, 1:]\n",
    "plt_y = points.transpose()[1, 1:]\n",
    "fig = plt.figure(figsize=(10, 250))\n",
    "ax = fig.subplots()\n",
    "ax.scatter(plt_x, plt_y)\n",
    "\n",
    "for i, txt in enumerate([_ for _ in tokenizer.word_index]):\n",
    "    if i%5 == 0:\n",
    "        ax.annotate(txt, (plt_x[i], plt_y[i]))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
